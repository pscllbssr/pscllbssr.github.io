---
title: "Wie analysiert man 120'000 TikTok-Videos, ohne selbst wochenlang zu scrollen?"
description: "Ein Blick in den ğŸ¤– Maschinenraum hinter unserer Tiktok-Recherche (Â«Der toxische Sog der ManosphereÂ»), vom Appium-Bot bis GPT-4o-Klassifizierung."
pubDate: 2025-12-12
tags: ["Datenjournalismus", "LLM", "Python"]
heroImage: "/images/blog/tiktok-automatisierung.jpeg"
---

## Simulation der User
Unsere fÃ¼nf Â«jungen MÃ¤nnerÂ» aus dem Experiment hatten jeweils ein eigenes ğŸ“± Smartphone. Zuvor erstellten wir manuell Accounts fÃ¼r sie und wÃ¤hlten stereotypisch mÃ¤nnliche Interessen aus der TikTok-Auswahl (Fussball, Gaming, Auto â€¦).

Um dem Algorithmus zusÃ¤tzliche Hinweise zu geben, fÃ¼hrten wir auf vier der fÃ¼nf Konten manuelle Suchen durch, nach Fitness- (Â«sixpackÂ») oder Business-Themen (Â«reich werdenÂ»).

Danach lief das Scrollen automatisch ğŸ”„: Ãœber ein verbundenes Laptop steuerten wir die Smartphones per Skript und Cronjobs. Mit [Appium](https://appium.io/docs/en/latest/) lassen sich Gesten und Interaktionen simulieren, als wÃ¼rden echte Nutzer die GerÃ¤te bedienen. Gleichzeitig kann man damit auch Inhalte aus der App auslesen, wie beispielsweise die Videobeschreibungen. Der Bot konnte so auf Videos mit bestimmten SignalwÃ¶rtern (z.â€¯B. Â« MuskelaufbauÂ» oder Â«GrindsetÂ») lÃ¤nger verweilen und ğŸ‘ Likes verteilen, Tiktok erkannte so unser Interesse am Thema. 

Ein Kontroll-Account blieb unverÃ¤ndert: keine manuelle Suche, keine Reaktionen auf Videos.

## Daten sammeln
Tiktok protokolliert praktisch jede Bewegung seiner Nutzer ğŸ“Š. Ein Teil dieser Daten lÃ¤sst sich herunterladen, unter anderem auch die Â«video watch historyÂ» ([Tiktok - Requesting your data](https://support.tiktok.com/en/account-and-privacy/personalized-ads-and-data/requesting-your-data)). Sie enthÃ¤lt Zeitstempel und IDs aller angesehenen Videos. 

Mit der Bibliothek [pyktok](https://github.com/dfreelon/pyktok/tree/main) lassen sich die Video-IDs um Metadaten wie Beschreibung, Views, Autor etc. erweitern und auch die Originalvideos â¬‡ï¸ herunterladen. Da Pyktok fÃ¼r jedes Video eine Anfrage an die TikTok-Server sendet, lohnt es sich bei ~120â€™000 Videos, die Anfragen zeitlich zu staffeln, um nicht als Bot erkannt und blockiert zu werden.

Downloads sollten zudem zeitnah erfolgen, falls Videos zwischenzeitlich gelÃ¶scht oder gesperrt werden. In den Metadaten finden sich zudem Ã¶ffentliche Links zu Untertiteln und weiteren Daten, die ebenfalls mit einem Skript abgerufen werden kÃ¶nnen.

## Auswertung & Analyse
Nun galt es einen Ãœberblick Ã¼ber den Datenberg zu gewinnen und herauszufinden, welche Inhalte der Manosphere zugeordnet werden kÃ¶nnen - ohne selbst alle 120â€™000 Videos anschauen zu mÃ¼ssen.

Mit den Video-Beschreibungen, insbesondere den enthaltenen Hashtags, kriegt man eine grobe Ãœbersicht. Insbesondere sahen wir hier, was andere vor uns auch schon gesehen haben: Der Tiktok-Algorithmus reagiert extrem schnell auf die Signale der User (siehe beispielsweise [Inside Tiktoks highly secretive algorithm - Wall Street Journal](https://www.wsj.com/video/series/inside-tiktoks-highly-secretive-algorithm) oder [Tiktok's muscle power - NRK](https://www.nrk.no/ostfold/tiktok_s-muscle-power_-how-children-are-drawn-into-a-world-of-extreme-exercise-1.16212259)). Bald nachdem die Bots Videos mit den Signal-WÃ¶rtern lÃ¤nger angeschaut und geliked hatten, enthielt jeder zweite Beitrag in ihrem Feed eines dieser WÃ¶rter. 

Die Manosphere umfasst viele Nischen, die zwar Antifeminismus und mÃ¤nnliche Dominanz teilen, aber oft eigene Codes und AusdrÃ¼cke verwenden. Viele Videos haben keine oder wenig aussagekrÃ¤ftige Beschreibung, sodass zusÃ¤tzlicher Kontext nÃ¶tig ist.

Deshalb bÃ¼ndelten wir unser Wissen Ã¼ber die Manosphere in einem Prompt und liessen eine KI die Videos kodieren. Ã„hnliche AnsÃ¤tze wurden auch schon wissenschaftlich erprobt, siehe beispielsweise [ChatGPT outperforms crowd workers for text-annotation tasks](https://doi.org/10.1073/pnas.2305016120) (Gilardi et al., 2023) oder auch [From Walls to Windows: Creating Transparency to Understand Filter Bubbles in Social Media](https://www.alexandria.unisg.ch/entities/publication/f41060db-3d3b-4071-ad92-8ac6bb1cc734) (Luka Bekavac et al., 2024). Wir nutzten das multimodale GPT-4o, dem wir nicht nur ğŸ“„ Text (Beschreibung, Untertitel, Sticker), sondern auch Bilder ğŸ–¼ï¸ (drei Standbilder pro Video, wo vorhanden) Ã¼bergaben. So konnte die KI Text- und Bildebene kombinieren.

Die KI lieferte nicht nur die Klassifizierung, sondern auch eine BegrÃ¼ndung inkl. Bildbeschreibung â€“ hilfreich fÃ¼r Nachvollziehbarkeit und zur Reduktion von Halluzinationen (â€œChain-of-thought Promptingâ€). Ãœber ein Skript liess sich das Modell bequem per API ansteuern und maschinenlesbaren Text ausgeben.

Bei solchen Datenmengen lohnt es sich, die Kosten im Auge zu behalten ğŸ’°. Ein kleiner Testlauf hilft, die ungefÃ¤hren Ausgaben abzuschÃ¤tzen.

Klar war auch, dass wir die Antworten der KI nicht einfach ungeprÃ¼ft Ã¼bernehmen konnten. Deshalb haben wir mehrere Massnahmen ergriffen:

- Temperatur reduziert: Wir haben die Modelltemperatur gesenkt, um konsistentere Ergebnisse zu erhalten.
- Interrater-ReliabilitÃ¤t geprÃ¼ft: Wir verglichen die EinschÃ¤tzungen der KI mit unseren eigenen (drei Personen) auf einer Stichprobe. Die Ãœbereinstimmung lag hoch (0,77 Krippendorffs Alpha) und war vergleichbar mit der Ãœbereinstimmung zwischen uns. Dabei fiel uns auch auf, dass die KI leicht konservativer klassifizierte als wir.
- Intrarater-ReliabilitÃ¤t geprÃ¼ft: Wir schickten Videos aus einer Stichprobe mehrfach an die KI. Die Ãœbereinstimmung war nahezu perfekt (0,98 Krippendorffs Alpha) und die Antworten damit konsistent.
- BegrÃ¼ndungen analysiert: In einer Stichprobe prÃ¼ften wir, wie die KI argumentierte. Sie folgte dabei Ã¤hnlichen SchlÃ¼ssen wie wir und traf vergleichbare Entscheidungen.

Diese Recherche hat doch einiges an Zeit â±ï¸ gekostet, trotz KI und Automatisierung. Die Technologien erlaubten uns zwar, grosse Mengen an Daten zu analysieren, brauchten aber gleichzeitig eine enge Begleitung: Wir mussten den Maschinen genau auf die Finger schauen, oft 2â€“3 Mal nachjustieren und viel ausprobieren ğŸ”§. Sicherlich hÃ¤tten wir manches noch anders machen oder optimieren kÃ¶nnen.